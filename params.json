{"name":"Fussy","tagline":"Javascript recommendation engine","body":"node-fussy\r\n==========\r\n\r\n*javascript recommendation engine*\r\n\r\nNOTE: I'm not an English native, so feel free to open issues if you see typos and bad grammar\r\n\r\n## Summary\r\n\r\nFussy is a recommendation engine, powered by user actions.\r\n\r\nIt does recommendation by binding likes and dislikes to patterns of a documents.\r\n\r\nA like will reinforce the bound between two patterns (can be a word, or a sequence a word; a concept), while a dislike will weaken it.\r\n\r\nThe longer the patterns, the more complex the network, and accurate the results will get, but at the cost of increasing memory and cpu usage.\r\nFor now I am using patterns of length 3 by default, enough to catch basic expressions and simple concepts.\r\n\r\nThis network is then evaluated against a new content, to compute a matching score.\r\nPositive memories will thus increase the score of the document, and bad memories will decrease it.\r\n\r\nSince the algorith doesn't use similarity between users (this is easily computable, though),\r\nyou can do recommendations of content even with one user.\r\n\r\nTo simulate a \"people in the same box than you also liked..\" recommendation, you just have to pick the next alternatives in the sorted results.\r\n\r\n\r\n[![NPM](https://nodei.co/npm/fussy.png?downloads=true&stars=true)](https://nodei.co/npm/fussy/)\r\n\r\n## Usage\r\n\r\nYou can see the examples/ dir, or:\r\n\r\n### Basic demo\r\n\r\n```javascript\r\nvar fussy = require('fussy');\r\nvar samples = [\r\n  {\r\n    profile: 'test_user_1', signal: fussy.POSITIVE,\r\n    content: \"a video advertisement about an upcoming movie featuring pirates\"\r\n  }, {\r\n    profile: 'test_user_1',  signal: fussy.POSITIVE,\r\n    content: \"a youtube ad about hackers\"\r\n  }, {\r\n    profile: 'test_user_1',  signal: fussy.POSITIVE,\r\n    content: \"a facebook ad selling cloud hosting\"\r\n  }, {\r\n    profile: 'test_user_1', signal: fussy.NEGATIVE,\r\n    content: \"a video advertisement featuring video games\"\r\n  }, {\r\n    profile: 'test_user_1', signal: fussy.NEGATIVE,\r\n    content: \"a facebook ad about video games\"\r\n  }, {\r\n    profile: 'test_user_2',  signal: fussy.POSITIVE,\r\n    content: \"a video advertisement about an upcoming movie featuring cowboys\"\r\n  }, {\r\n    profile: 'test_user_2',  signal: fussy.NEGATIVE,\r\n    content: \"a movie trailer about bearded magicians\"\r\n  }, {\r\n    profile: 'test_user_2', signal: fussy.NEGATIVE,\r\n    content: \"a facebook ad selling cloud hosting\"\r\n  }, {\r\n    profile: 'test_user_2', signal: fussy.POSITIVE,\r\n    content: \"a trailer for movie featuring cowboy sharks against aliens\"\r\n  }, {\r\n    profile: 'test_user_2', signal: fussy.POSITIVE,\r\n    content: \"a facebook ad about a new farm game\"\r\n  }\r\n];\r\n\r\n// var engine = new fussy.Engine(\"./database.json\");\r\n// OR\r\nvar engine = new fussy.Engine({\r\n  stringSize: [3, 14],\r\n  ngramSize: 3\r\n});\r\n\r\nfor (var i=0 ; i < samples.length ; i++) {\r\n  engine.pushEvent(samples[i]);\r\n}\r\n\r\n// remove noise, by filtering weakest connections between concepts\r\nengine.prune(-2, 2);\r\n\r\n\r\nengine.rateProfiles('an ad showing a video game about pirates', { \r\n  profiles: ['test_user_1', \r\n             'test_user_2'] \r\n});\r\n\r\nengine.rateProfiles(\r\n 'an ad showing a video game about pirates', { limit: 2 }\r\n);\r\n// [ [\"test_user_2\",14], [\"test_user_1\",-9] ]\r\n\r\nengine.rateContents('test_user_1', [\r\n  'an ad about magicians',\r\n  'an ad about tablet games'\r\n]);\r\n// [ [\"an ad about magicians\",0], [\"an ad about tablet games\",-3] ]\r\n\r\n\r\nengine.rateContents('test_user_2', [\r\n  'an ad about magicians',\r\n  'an ad about tablet games'\r\n]);\r\n//  [ [\"an ad about tablet games\",0], [\"an ad about magicians\",-2] ]\r\n\r\n\r\nengine.save(\"database.json\");\r\n```\r\n\r\n## Remarks / features\r\n\r\n\r\n### Data-efficient\r\n\r\nEven if data is scarce, the use of weak relationship (for the moment only synonyms) improve results even when there is not a lot of data to be trained on (eg. less than 10). there will be errors, but it should still perform better than with no data at all.\r\n\r\n### Self-regulating\r\n\r\nThe network can change over time. New connections can be created, old ones can be reinforced or deleted. A user can choose to hate something he used to love months ago.\r\n\r\n### Flexible\r\n\r\nYou can put anything in the content string, eg meta-keyword to describe not only the content but also the context or environment. \r\n\r\nYou are not limited to -1 or +1, you can use any value. For instance if the user likes a product, that could be a +1, and if he buys it, a +5. This is up to you, you should do AB testing or other research to tweak this.\r\nJust remember that a signal value of 0 will have no effect, because it represents the non-action (eg. the user just ignore the ad, or skip a product evaluation). If you want to force a link to be weakened (eg. automatically, after a few days or weeks) you have to use a negative value.\r\n\r\n\r\n## Known issues\r\n\r\n * It would work better with a filter for the most common (and thus irrelevant) keywords. Than can be implemented using some kind of TF-IDF-like algorithm, but I've just not done it yet. This is the next thing on the TODO.\r\n\r\n * Injecting additional, weak connections is powerful, but using a thesaurus is still a bit limited. It would work even better with network data from DBpedia's ontology, or other semantic graph databases.\r\n\r\n * The memory usage can be a problem, because a lot of data is stored per-user. You will eventually need to purge the network from time to time, in order to remove the weakest/old links. That could be automatic, but it depends on how much memory you can use, so it won't be easy to implement this kind of GC.\r\n\r\n * The profile networks are not easily human-readable. They are made by and for the machine. But you could try to export them to visualize the graph in Gephi for instance.\r\n\r\n## Documentation\r\n\r\n### new fussy.Engine()\r\n\r\nInstanciate a new engine with default settings\r\n\r\n### new fussy.Engine(file_path)\r\n\r\nInstanciate a new engine by loading a data dump file (basically a collection of profiles + the config)\r\n\r\n### new fussy.Engine({ ngramSize: 3 }}\r\n\r\nInstanciate a new engine, with a ngramSize of 3. Values > 3 go deeper into the meaning of the sentence (complex word patterns are added to the network), but dramatically increase the memory andcpu usage. \"a n-gram size of 3 should be enough for most people\" â„¢.\r\n\r\n### fussy.Engine#rateContents(profile_id, array_of_content_strings)\r\n\r\nEvaluate the interest of one profile for a set of contents.\r\n\r\n### fussy.Engine#rateProfiles(content[, { profiles: array_of_profile_ids, limit: n_top_profiles_to_keep}])\r\n\r\nSearch for profiles that could be the most interested by a givent content.\r\n\r\n### fussy.Engine#prune(min_weight, max_weight)\r\n\r\nDelete non-significant links (those near 0 influence)\r\n\r\n###fussy.Engine#save(file_path)\r\n\r\nDump the databse to a JSON file\r\n\r\n### fussy.clearContent(string)\r\n\r\nThe cleaning function used internally by Fussy. It remove useless spaces and line returns. Handy to clean tweets to be printed in the console, for instance.\r\n\r\n## TODO\r\n\r\n * Add a TF-IDF algorithm?\r\n * MongoDB implementation?\r\n\r\n## Changelog\r\n\r\n#### 0.0.4\r\n\r\n * Small optimization for the rateContents method\r\n\r\n#### 0.0.3\r\n\r\n * Added the rateProfiles and rateContents methods\r\n\r\n#### 0.0.2\r\n\r\n * Total rewrite. See README for more information. Yup, this file.\r\n\r\n#### 0.0.1 (Wednesday, December 5, 2012)\r\n\r\n * Added `database.size`\r\n * Added `database.prune(threshold)`\r\n * Added `database.toFile(fileName)`\r\n * Removed the toy Twitter database from core\r\n * Added an example crawler you could use to build a tag database\r\n\r\n#### 0.0.0\r\n\r\n * Initial version, not very documented\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}